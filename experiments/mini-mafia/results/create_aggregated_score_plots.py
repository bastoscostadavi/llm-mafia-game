#!/usr/bin/env python3
"""
Z-Score Aggregated Benchmark Plot Generator

Creates standardized performance plots by:
1. Loading data from JSON files generated by create_benchmark_plots.py  
2. Computing z-scores for each model relative to background mean/std
3. Aggregating z-scores across all backgrounds for each behavior type
4. Creating final benchmark plots with standardized scores

Behavior types:
- Deceive: mafioso_*.json files (evil win rates)
- Detect: detective_*.json files (good win rates) 
- Disclose: villager_*.json files (good win rates)
"""

import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from collections import defaultdict
import os
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from PIL import Image

def load_company_logo(company, size=(40, 40)):
    """Load actual company logos from the logos folder"""
    # Map company names to logo filenames  
    logo_files = {
        "OpenAI": "openai.png",
        "X": "xai.png",
        "Mistral AI": "mistral.png", 
        "Meta": "meta.png",
        "Alibaba": "baba.png",
        "Anthropic": "anthropic.png",
        "DeepMind": "deepmind.png",
        "DeepSeek": "deepseek.png",
        "Google": "deepmind.png"  # Use DeepMind logo for Google since they're the same company
    }
    
    try:
        logo_path = f"logos/{logo_files[company]}"
        if os.path.exists(logo_path):
            # Load and resize the actual logo
            img = Image.open(logo_path)
            
            # Convert to RGBA if needed
            if img.mode != 'RGBA':
                img = img.convert('RGBA')
            
            # Resize maintaining aspect ratio
            img.thumbnail(size, Image.Resampling.LANCZOS)
            
            # Create new image with transparent background
            new_img = Image.new('RGBA', size, (255, 255, 255, 0))
            
            # Center the logo
            x = (size[0] - img.width) // 2
            y = (size[1] - img.height) // 2
            new_img.paste(img, (x, y), img if img.mode == 'RGBA' else None)
            
            return np.array(new_img)
        else:
            print(f"Logo file not found: {logo_path}")
            return None
            
    except Exception as e:
        print(f"Error loading logo for {company}: {e}")
        return None

def load_data_files(behavior_type, exclude_background=None):
    """Load all JSON data files for a given behavior type"""
    pattern = f"{behavior_type}_*_data.json"
    data_files = list(Path(".").glob(pattern))
    
    if not data_files:
        raise FileNotFoundError(f"No data files found for pattern: {pattern}")
    
    datasets = []
    for file_path in sorted(data_files):
        with open(file_path, 'r') as f:
            data = json.load(f)
            
            # Skip specified background if requested
            if exclude_background and exclude_background.lower() in data['background'].lower():
                continue
                
            datasets.append({
                'filename': file_path.name,
                'background': data['background'],
                'models': data['models'],
                'win_rates': data['win_rates'],
                'errors': data['errors'],
                'companies': data['companies']
            })
    
    return datasets

def compute_z_scores(datasets):
    """Compute z-scores and their uncertainties for each model across all backgrounds"""
    # Collect all z-scores and their uncertainties for each model
    model_z_scores = defaultdict(list)
    model_z_uncertainties = defaultdict(list)  # Store z-score uncertainties
    model_companies = {}  # Store company mapping
    
    for dataset in datasets:
        models = dataset['models']
        win_rates = np.array(dataset['win_rates'])
        errors = np.array(dataset['errors'])  # These are the Bayesian standard deviations
        companies = dataset['companies']
        
        # Store company info for each model
        for model, company in zip(models, companies):
            model_companies[model] = company
        
        # Compute mean and std across all models in this background
        mean_rate = np.mean(win_rates)
        std_rate = np.std(win_rates, ddof=1)  # Sample standard deviation
        
        if std_rate == 0:
            print(f"Warning: Zero std deviation for background {dataset['background']}, skipping")
            continue
        
        # Compute z-scores for each model
        z_scores = (win_rates - mean_rate) / std_rate
        
        # Compute z-score uncertainties via error propagation
        # δz = δ(win_rate) / std_rate  (treating std_rate as exact for simplicity)
        z_uncertainties = errors / std_rate
        
        # Store z-scores and their uncertainties by model
        for model, z_score, z_uncertainty in zip(models, z_scores, z_uncertainties):
            model_z_scores[model].append(z_score)
            model_z_uncertainties[model].append(z_uncertainty)
    
    return model_z_scores, model_z_uncertainties, model_companies

def aggregate_z_scores(model_z_scores, model_z_uncertainties):
    """Aggregate z-scores across backgrounds and compute proper propagated uncertainties"""
    aggregated_results = {}
    
    for model, z_scores in model_z_scores.items():
        z_array = np.array(z_scores)
        z_uncert_array = np.array(model_z_uncertainties[model])
        
        # Aggregate: mean z-score across backgrounds
        mean_z_score = np.mean(z_array)
        
        # Proper uncertainty propagation for the mean: δz_mean = √(Σδz_i²)/n
        propagated_uncertainty = np.sqrt(np.sum(z_uncert_array**2)) / len(z_array)
        
        aggregated_results[model] = {
            'mean_z_score': mean_z_score,
            'sem_z_score': propagated_uncertainty,
            'n_backgrounds': len(z_array)
        }
    
    return aggregated_results

def create_zscore_plot(aggregated_results, model_companies, behavior_type, filename, excluded_background=None):
    """Create horizontal bar plot with z-scores"""
    # Use non-interactive backend
    plt.ioff()
    
    # Set font size to match existing plots
    plt.rcParams.update({
        'font.size': 24,
        'axes.labelsize': 24,
        'axes.titlesize': 24,
        'xtick.labelsize': 24,
        'ytick.labelsize': 24,
        'legend.fontsize': 24,
        'figure.titlesize': 24
    })
    
    # Sort models by mean z-score (ascending) so best performer appears at top of plot
    sorted_models = sorted(aggregated_results.keys(), 
                          key=lambda x: aggregated_results[x]['mean_z_score'], 
                          reverse=False)
    
    # Extract data for plotting (reversed order so highest scores appear at top)
    models = sorted_models
    z_scores = [aggregated_results[model]['mean_z_score'] for model in models]
    z_errors = [aggregated_results[model]['sem_z_score'] for model in models]
    companies = [model_companies.get(model, 'Unknown') for model in models]
    
    fig, ax = plt.subplots(figsize=(14, 7))
    
    y_positions = range(len(models))
    
    # Create bars - use neutral color for z-scores
    bars = ax.barh(y_positions, z_scores, xerr=z_errors, 
                   color='#4A90E2', alpha=0.8, height=0.6,
                   error_kw={'capsize': 5, 'capthick': 2})
    
    # Add model names on the right side of bars (without values)
    for i, (model, z_score, z_error) in enumerate(zip(models, z_scores, z_errors)):
        ax.text(max(z_score + z_error + 0.1, 0.1), i, 
                f'{model}', 
                ha='left', va='center', fontweight='bold', fontsize=24)
    
    # Add company logos on the left
    # logo_x_pos = -2.2  # Fixed position to the left of -2
    # for i, company in enumerate(companies):
    #     logo_img = load_company_logo(company, size=(40, 40))
    #     if logo_img is not None:
    #         try:
    #             imagebox = OffsetImage(logo_img, zoom=0.8)
    #             ab = AnnotationBbox(imagebox, (logo_x_pos, i), frameon=False, 
    #                               xycoords='data', boxcoords="data")
    #             ax.add_artist(ab)
    #         except Exception as e:
    #             print(f"Failed to add logo for {company}: {e}")
    #             # Fallback to company initial
    #             ax.text(logo_x_pos, i, company[0], ha='center', va='center', 
    #                     fontweight='bold', fontsize=24, color='black',
    #                     bbox=dict(boxstyle="circle,pad=0.3", facecolor='lightgray'))
    #     else:
    #         # Fallback to company initial
    #         ax.text(logo_x_pos, i, company[0], ha='center', va='center', 
    #                 fontweight='bold', fontsize=24, color='black',
    #                 bbox=dict(boxstyle="circle,pad=0.3", facecolor='lightgray'))
    
    # Set axis labels based on behavior type
    behavior_labels = {
        'Deceive': 'Deceive Score',
        'Detect': 'Detect Score', 
        'Disclose': 'Disclose Score'
    }
    xlabel = behavior_labels.get(behavior_type, 'Performance Score')
    ax.set_xlabel(xlabel, fontsize=24, fontweight='bold')
    ax.set_yticks([])  # Remove y-axis labels
    
    # Set fixed x-axis limits from -2.5 to 2.5
    ax.set_xlim(-2.5, 2.5)
    
    # Set x-axis ticks from -2.5 to 2.5
    ax.set_xticks(np.arange(-2.5, 3, 0.5))  # Ticks at -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5
    ax.xaxis.set_tick_params(labelsize=24)
    
    # Add vertical line at z=0
    ax.axvline(x=0, color='gray', alpha=0.5, linewidth=1, linestyle='--')
    
    # Add custom grid lines in the -2.5 to 2.5 range
    for x in np.arange(-2.5, 3, 0.5):
        if x != 0:  # Don't double-draw the zero line
            ax.axvline(x=x, color='gray', alpha=0.2, linewidth=0.5)
    
    # Hide all spines like benchmark plots
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)  # Hide default bottom spine
    
    # Add custom bottom line from -2.5 to 2.5
    ax.plot([-2.5, 2.5], [ax.get_ylim()[0], ax.get_ylim()[0]], color='black', linewidth=0.8)
    
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close()
    print(f"Z-score plot saved as {filename}")
    
    # Return summary statistics
    return {
        'models': models,
        'z_scores': z_scores,
        'z_errors': z_errors,
        'n_models': len(models)
    }

def create_exponentiated_plot(aggregated_results, model_companies, behavior_type, filename, excluded_background=None):
    """Create horizontal bar plot with exponentiated z-scores to make all values positive"""
    # Use non-interactive backend
    plt.ioff()
    
    # Set font size to match existing plots
    plt.rcParams.update({
        'font.size': 24,
        'axes.labelsize': 24,
        'axes.titlesize': 24,
        'xtick.labelsize': 24,
        'ytick.labelsize': 24,
        'legend.fontsize': 24,
        'figure.titlesize': 24
    })
    
    # Sort models by exponentiated z-score (ascending) so best performer appears at top of plot
    sorted_models = sorted(aggregated_results.keys(), 
                          key=lambda x: np.exp(aggregated_results[x]['mean_z_score']), 
                          reverse=False)
    
    # Extract data for plotting
    models = sorted_models
    z_scores = [aggregated_results[model]['mean_z_score'] for model in models]
    z_errors = [aggregated_results[model]['sem_z_score'] for model in models]
    companies = [model_companies.get(model, 'Unknown') for model in models]
    
    # Exponentiate the z-scores and propagate uncertainties
    exp_scores = [np.exp(z) for z in z_scores]
    # Error propagation for exp(z): d(exp(z)) = exp(z) * dz
    exp_errors = [np.exp(z) * err for z, err in zip(z_scores, z_errors)]
    
    fig, ax = plt.subplots(figsize=(14, 7))
    
    y_positions = range(len(models))
    
    # Create bars - use different color for exponentiated scores
    bars = ax.barh(y_positions, exp_scores, xerr=exp_errors, 
                   color='#E74C3C', alpha=0.8, height=0.6,
                   error_kw={'capsize': 5, 'capthick': 2})
    
    # Add model names on the right side of bars
    for i, (model, exp_score, exp_error) in enumerate(zip(models, exp_scores, exp_errors)):
        ax.text(exp_score + exp_error + 0.05, i, 
                f'{model}', 
                ha='left', va='center', fontweight='bold', fontsize=24)
    
    # Add company logos on the left
    # logo_x_pos = -0.15  # Adjust position for exponentiated scale
    # for i, company in enumerate(companies):
    #     logo_img = load_company_logo(company, size=(40, 40))
    #     if logo_img is not None:
    #         try:
    #             imagebox = OffsetImage(logo_img, zoom=0.8)
    #             ab = AnnotationBbox(imagebox, (logo_x_pos, i), frameon=False, 
    #                               xycoords='data', boxcoords="data")
    #             ax.add_artist(ab)
    #         except Exception as e:
    #             print(f"Failed to add logo for {company}: {e}")
    #             # Fallback to company initial
    #             ax.text(logo_x_pos, i, company[0], ha='center', va='center', 
    #                     fontweight='bold', fontsize=24, color='black',
    #                     bbox=dict(boxstyle="circle,pad=0.3", facecolor='lightgray'))
    #     else:
    #         # Fallback to company initial
    #         ax.text(logo_x_pos, i, company[0], ha='center', va='center', 
    #                 fontweight='bold', fontsize=24, color='black',
    #                 bbox=dict(boxstyle="circle,pad=0.3", facecolor='lightgray'))
    
    # Set axis labels based on behavior type
    behavior_labels = {
        'Deceive': 'Deceive Score',
        'Detect': 'Detect Score', 
        'Disclose': 'Disclose Score'
    }
    xlabel = behavior_labels.get(behavior_type, 'Performance Score')
    ax.set_xlabel(xlabel, fontsize=24, fontweight='bold')
    ax.set_yticks([])  # Remove y-axis labels
    
    # Set data-driven x-axis limits with some padding
    max_val = max([s + e for s, e in zip(exp_scores, exp_errors)])
    min_val = min([s - e for s, e in zip(exp_scores, exp_errors)])
    padding = (max_val - min_val) * 0.1
    x_min = max(0, min_val - padding)
    x_max = max_val + padding
    ax.set_xlim(x_min, x_max)
    
    # Add vertical line at exp(0) = 1 - the key reference point
    if x_min <= 1 <= x_max:  # Only show if 1 is in the visible range
        ax.axvline(x=1, color='gray', alpha=0.7, linewidth=2, linestyle='--')
    
    # Minimal, clean x-axis ticks - let matplotlib choose reasonable ones
    # But ensure 1 is included if it's in range
    if x_min <= 1 <= x_max:
        # Get matplotlib's suggested ticks
        suggested_ticks = ax.get_xticks()
        # Add 1 if it's not already close to an existing tick
        ticks = list(suggested_ticks)
        if not any(abs(t - 1) < 0.1 for t in ticks):
            ticks.append(1)
        ticks = [t for t in ticks if x_min <= t <= x_max]
        ticks.sort()
        ax.set_xticks(ticks)
    ax.xaxis.set_tick_params(labelsize=24)
    
    # Hide all spines like benchmark plots
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)  # Hide default bottom spine
    
    # Add custom bottom line
    ax.plot([x_min, x_max], [ax.get_ylim()[0], ax.get_ylim()[0]], color='black', linewidth=0.8)
    
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close()
    print(f"Exponentiated plot saved as {filename}")
    
    # Return summary statistics
    return {
        'models': models,
        'exp_scores': exp_scores,
        'exp_errors': exp_errors,
        'n_models': len(models)
    }

def main():
    """Generate aggregated z-score plots for all behavior types"""
    behavior_types = {
        'mafioso': {'name': 'Deceive', 'description': 'Mafioso performance across backgrounds'},
        'detective': {'name': 'Disclose', 'description': 'Detective performance across backgrounds'},
        'villager': {'name': 'Detect', 'description': 'Villager performance across backgrounds'}
    }
    
    # Create plots with all backgrounds
    print("🔄 Creating aggregated z-score benchmark plots (all backgrounds)...")
    
    for behavior_key, behavior_info in behavior_types.items():
        print(f"\n📊 Processing {behavior_info['name']} ({behavior_key}) data...")
        
        try:
            # Load data files
            datasets = load_data_files(behavior_key, exclude_background=None)
            print(f"   Loaded {len(datasets)} background datasets")
            
            # Compute z-scores
            model_z_scores, model_z_uncertainties, model_companies = compute_z_scores(datasets)
            print(f"   Computed z-scores for {len(model_z_scores)} models")
            
            # Aggregate across backgrounds
            aggregated_results = aggregate_z_scores(model_z_scores, model_z_uncertainties)
            print(f"   Aggregated results across backgrounds")
            
            # Create plot
            filename = f"{behavior_key}_score_benchmark.png"
            summary = create_zscore_plot(aggregated_results, model_companies, 
                                       behavior_info['name'], filename)
            
            # Create exponentiated plot
            exp_filename = f"{behavior_key}_score_benchmark_exponential.png"
            exp_summary = create_exponentiated_plot(aggregated_results, model_companies, 
                                                   behavior_info['name'], exp_filename)
            
            # Print summary
            print(f"   📈 Created plot with {summary['n_models']} models")
            print(f"   📋 Top performer: {summary['models'][0]} (z-score: {summary['z_scores'][0]:.2f})")
            print(f"   📈 Created exponentiated plot with {exp_summary['n_models']} models")
            
            # Print exponentiated scores for table verification
            print(f"   📊 Exponentiated scores for {behavior_info['name']}:")
            for i, model in enumerate(exp_summary['models']):
                score = exp_summary['exp_scores'][i]
                error = exp_summary['exp_errors'][i] 
                print(f"      {model}: {score:.2f} ± {error:.2f}")
            
        except Exception as e:
            print(f"   ❌ Error processing {behavior_key}: {e}")
            continue
    
    # Create additional Deceive plots excluding each background one at a time
    print("\n🔄 Creating Deceive robustness plots (excluding each background)...")
    
    # Only create these additional plots for Deceive (mafioso)
    behavior_key = 'mafioso'
    behavior_info = behavior_types[behavior_key]
    
    # Define backgrounds to exclude for robustness testing
    backgrounds_to_exclude = [
        'GPT-4.1 Mini',
        'GPT-5 Mini', 
        'Grok 3 Mini',
        'DeepSeek V3.1'
    ]
    
    for exclude_bg in backgrounds_to_exclude:
        print(f"\n📊 Processing {behavior_info['name']} excluding {exclude_bg}...")
        
        try:
            # Load data files excluding specified background
            datasets = load_data_files(behavior_key, exclude_background=exclude_bg)
            print(f"   Loaded {len(datasets)} background datasets (excluding {exclude_bg})")
            
            if len(datasets) == 0:
                print(f"   ⚠️  No datasets remaining after excluding {exclude_bg}, skipping")
                continue
                
            # Compute z-scores
            model_z_scores, model_z_uncertainties, model_companies = compute_z_scores(datasets)
            print(f"   Computed z-scores for {len(model_z_scores)} models")
            
            # Aggregate across backgrounds
            aggregated_results = aggregate_z_scores(model_z_scores, model_z_uncertainties)
            print(f"   Aggregated results across remaining backgrounds")
            
            # Create plot
            safe_bg_name = exclude_bg.replace(' ', '_').replace('.', '_').replace('-', '_')
            filename = f"{behavior_key}_score_benchmark_no_{safe_bg_name.lower()}.png"
            summary = create_zscore_plot(aggregated_results, model_companies, 
                                       behavior_info['name'], filename, exclude_bg)
            
            # Create exponentiated plot
            exp_filename = f"{behavior_key}_score_benchmark_no_{safe_bg_name.lower()}_exponential.png"
            exp_summary = create_exponentiated_plot(aggregated_results, model_companies, 
                                                   behavior_info['name'], exp_filename, exclude_bg)
            
            # Print summary
            print(f"   📈 Created plot with {summary['n_models']} models")
            print(f"   📋 Top performer: {summary['models'][0]} (z-score: {summary['z_scores'][0]:.2f})")
            print(f"   📈 Created exponentiated plot with {exp_summary['n_models']} models")
            
        except Exception as e:
            print(f"   ❌ Error processing {behavior_key} excluding {exclude_bg}: {e}")
            continue
    
    print("\n✅ Z-score aggregation complete!")
    print("📁 Generated files:")
    for behavior_key in behavior_types.keys():
        filename = f"{behavior_key}_score_benchmark.png"
        exp_filename = f"{behavior_key}_score_benchmark_exponential.png"
        if os.path.exists(filename):
            print(f"   - {filename}")
        if os.path.exists(exp_filename):
            print(f"   - {exp_filename}")
            
    # List robustness plots for Deceive
    backgrounds_to_exclude = ['GPT-4.1 Mini', 'GPT-5 Mini', 'Grok 3 Mini', 'DeepSeek V3.1']
    for exclude_bg in backgrounds_to_exclude:
        safe_bg_name = exclude_bg.replace(' ', '_').replace('.', '_').replace('-', '_')
        filename = f"mafioso_score_benchmark_no_{safe_bg_name.lower()}.png"
        exp_filename = f"mafioso_score_benchmark_no_{safe_bg_name.lower()}_exponential.png"
        if os.path.exists(filename):
            print(f"   - {filename}")
        if os.path.exists(exp_filename):
            print(f"   - {exp_filename}")

if __name__ == "__main__":
    main()